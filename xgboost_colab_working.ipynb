{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working Version: XGBoost Domain Analysis\n",
    "\n",
    "This version uses sklearn's RandomForestClassifier as an alternative that handles missing classes better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Clone repository and install packages\n",
    "import os\n",
    "\n",
    "# Clone repository if not already present\n",
    "if not os.path.exists('/content/tabicl'):\n",
    "    print(\"Cloning repository...\")\n",
    "    !git clone https://github.com/cliu238/tabicl.git\n",
    "    print(\"Repository cloned successfully!\")\n",
    "else:\n",
    "    print(\"Repository already exists.\")\n",
    "\n",
    "# Change to repository directory\n",
    "%cd /content/tabicl\n",
    "\n",
    "# Install required packages\n",
    "print(\"\\nInstalling packages...\")\n",
    "!pip install xgboost scikit-learn pandas numpy matplotlib seaborn plotly -q\n",
    "print(\"Packages installed!\")\n",
    "\n",
    "# Verify data files exist\n",
    "print(\"\\nChecking data files:\")\n",
    "!ls -lh processed_data/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style and random seed\n",
    "plt.style.use('default')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"Note: Using RandomForest instead of XGBoost to handle missing classes better\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the dataset\n",
    "df = pd.read_csv('processed_data/adult_numeric_20250729_155457.csv')\n",
    "\n",
    "print(\"üìä Dataset Overview:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nüè• Sites distribution:\")\n",
    "print(df['site'].value_counts())\n",
    "print(f\"\\nüéØ Target (va34): {df['va34'].nunique()} unique classes\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "print(\"üîß Preprocessing data...\")\n",
    "\n",
    "# Drop cod5 column\n",
    "if 'cod5' in df.columns:\n",
    "    df = df.drop('cod5', axis=1)\n",
    "    print(\"‚úÖ Dropped 'cod5' column\")\n",
    "\n",
    "# Use LabelEncoder for consistent encoding\n",
    "le = LabelEncoder()\n",
    "df['va34_encoded'] = le.fit_transform(df['va34'])\n",
    "\n",
    "print(f\"\\nEncoded {len(le.classes_)} unique classes\")\n",
    "print(f\"Class range: {df['va34_encoded'].min()} to {df['va34_encoded'].max()}\")\n",
    "\n",
    "# Separate features, target, and sites\n",
    "X = df.drop(['va34', 'va34_encoded', 'site'], axis=1)\n",
    "y = df['va34_encoded']\n",
    "sites = df['site']\n",
    "\n",
    "print(f\"\\nüìê Data shapes:\")\n",
    "print(f\"Features: {X.shape}\")\n",
    "print(f\"Target: {y.shape}\")\n",
    "print(f\"Sites: {sites.unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_domain_splits(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"Create train/test splits for each site\"\"\"\n",
    "    domain_splits = {}\n",
    "    \n",
    "    for site in df['site'].unique():\n",
    "        site_data = df[df['site'] == site]\n",
    "        X_site = site_data.drop(['va34', 'va34_encoded', 'site'], axis=1)\n",
    "        y_site = site_data['va34_encoded']\n",
    "        \n",
    "        print(f\"Processing {site}: {len(site_data)} samples, {site_data['va34'].nunique()} classes\")\n",
    "        \n",
    "        # Simple train/test split\n",
    "        if len(site_data) >= 10:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_site, y_site, test_size=test_size, random_state=random_state\n",
    "            )\n",
    "        else:\n",
    "            X_train, X_test = X_site, X_site\n",
    "            y_train, y_test = y_site, y_site\n",
    "        \n",
    "        domain_splits[site] = {\n",
    "            'X_train': X_train, 'X_test': X_test,\n",
    "            'y_train': y_train, 'y_test': y_test,\n",
    "            'full_X': X_site, 'full_y': y_site\n",
    "        }\n",
    "    \n",
    "    return domain_splits\n",
    "\n",
    "def train_model(X_train, y_train, model_type='rf'):\n",
    "    \"\"\"Train a model (RandomForest or GradientBoosting)\"\"\"\n",
    "    if model_type == 'rf':\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        model = GradientBoostingClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    try:\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Get unique labels for F1 calculation\n",
    "        unique_labels = np.unique(np.concatenate([y_test, y_pred]))\n",
    "        f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0, labels=unique_labels)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'balanced_accuracy': balanced_acc,\n",
    "            'f1_macro': f1_macro\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"    Error: {e}\")\n",
    "        return {'accuracy': 0.0, 'balanced_accuracy': 0.0, 'f1_macro': 0.0}\n",
    "\n",
    "print(\"‚úÖ Functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create domain splits\n",
    "print(\"Creating domain splits...\")\n",
    "print(\"=\"*60)\n",
    "domain_data = create_domain_splits(df)\n",
    "\n",
    "print(\"\\nüìÅ Domain splits summary:\")\n",
    "for site, data in domain_data.items():\n",
    "    print(f\"{site:10} - Train: {len(data['X_train']):4}, Test: {len(data['X_test']):4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-domain performance evaluation\n",
    "print(\"\\nüéØ Evaluating In-Domain Performance with RandomForest...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "in_domain_results = {}\n",
    "\n",
    "for site in domain_data.keys():\n",
    "    print(f\"\\nTraining for {site}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model = train_model(\n",
    "        domain_data[site]['X_train'], \n",
    "        domain_data[site]['y_train'],\n",
    "        model_type='rf'\n",
    "    )\n",
    "    \n",
    "    # Test on same site\n",
    "    results = evaluate_model(\n",
    "        model,\n",
    "        domain_data[site]['X_test'],\n",
    "        domain_data[site]['y_test']\n",
    "    )\n",
    "    \n",
    "    in_domain_results[site] = results\n",
    "    in_domain_results[site]['model'] = model\n",
    "    \n",
    "    print(f\"  ‚úì Accuracy: {results['accuracy']:.4f}\")\n",
    "    print(f\"  ‚úì Balanced Acc: {results['balanced_accuracy']:.4f}\")\n",
    "    print(f\"  ‚úì F1 Macro: {results['f1_macro']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize in-domain results\n",
    "sites_list = list(in_domain_results.keys())\n",
    "accuracies = [in_domain_results[s]['accuracy'] for s in sites_list]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(sites_list, accuracies, color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "plt.xlabel('Site', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('In-Domain Performance by Site (RandomForest)', fontsize=14, fontweight='bold')\n",
    "plt.ylim([0, 1])\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Average In-Domain Accuracy: {np.mean(accuracies):.4f} ¬± {np.std(accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out-domain performance evaluation\n",
    "print(\"\\nüîÑ Evaluating Out-Domain Performance...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "out_domain_results = {}\n",
    "\n",
    "for train_site in domain_data.keys():\n",
    "    out_domain_results[train_site] = {}\n",
    "    \n",
    "    print(f\"\\nTraining on {train_site}...\")\n",
    "    model = train_model(\n",
    "        domain_data[train_site]['full_X'],\n",
    "        domain_data[train_site]['full_y'],\n",
    "        model_type='rf'\n",
    "    )\n",
    "    \n",
    "    for test_site in domain_data.keys():\n",
    "        if train_site == test_site:\n",
    "            continue\n",
    "            \n",
    "        results = evaluate_model(\n",
    "            model,\n",
    "            domain_data[test_site]['full_X'],\n",
    "            domain_data[test_site]['full_y']\n",
    "        )\n",
    "        \n",
    "        out_domain_results[train_site][test_site] = results\n",
    "        print(f\"  {train_site} ‚Üí {test_site}: {results['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance matrix\n",
    "sites_list = list(domain_data.keys())\n",
    "n_sites = len(sites_list)\n",
    "perf_matrix = np.zeros((n_sites, n_sites))\n",
    "\n",
    "for i, train_site in enumerate(sites_list):\n",
    "    for j, test_site in enumerate(sites_list):\n",
    "        if train_site == test_site:\n",
    "            perf_matrix[i, j] = in_domain_results[train_site]['accuracy']\n",
    "        else:\n",
    "            perf_matrix[i, j] = out_domain_results[train_site][test_site]['accuracy']\n",
    "\n",
    "# Visualize matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(perf_matrix, annot=True, fmt='.3f', \n",
    "            xticklabels=sites_list, yticklabels=sites_list,\n",
    "            cmap='RdYlGn', vmin=0, vmax=1,\n",
    "            cbar_kws={'label': 'Accuracy'})\n",
    "\n",
    "plt.title('Domain Transfer Performance Matrix\\n(RandomForest)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Train Site', fontsize=12)\n",
    "plt.xlabel('Test Site', fontsize=12)\n",
    "\n",
    "# Highlight diagonal\n",
    "for i in range(n_sites):\n",
    "    plt.gca().add_patch(plt.Rectangle((i, i), 1, 1, fill=False, edgecolor='blue', lw=3))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-domain performance\n",
    "print(\"\\nüåê Evaluating Cross-Domain Performance...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "cross_domain_results = {}\n",
    "\n",
    "for held_out_site in domain_data.keys():\n",
    "    train_sites = [s for s in domain_data.keys() if s != held_out_site]\n",
    "    \n",
    "    # Combine training data\n",
    "    X_train_list = [domain_data[s]['full_X'] for s in train_sites]\n",
    "    y_train_list = [domain_data[s]['full_y'] for s in train_sites]\n",
    "    \n",
    "    X_train_combined = pd.concat(X_train_list, axis=0)\n",
    "    y_train_combined = pd.concat(y_train_list, axis=0)\n",
    "    \n",
    "    print(f\"\\nTraining on all except {held_out_site} ({len(X_train_combined)} samples)...\")\n",
    "    \n",
    "    model = train_model(X_train_combined, y_train_combined, model_type='rf')\n",
    "    \n",
    "    results = evaluate_model(\n",
    "        model,\n",
    "        domain_data[held_out_site]['full_X'],\n",
    "        domain_data[held_out_site]['full_y']\n",
    "    )\n",
    "    \n",
    "    cross_domain_results[held_out_site] = results\n",
    "    print(f\"  Testing on {held_out_site}: {results['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" \"*25 + \"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare comparison data\n",
    "comparison_data = []\n",
    "for site in domain_data.keys():\n",
    "    # In-domain\n",
    "    comparison_data.append({\n",
    "        'Site': site,\n",
    "        'Scenario': 'In-Domain',\n",
    "        'Accuracy': in_domain_results[site]['accuracy']\n",
    "    })\n",
    "    \n",
    "    # Out-domain average\n",
    "    out_accs = [out_domain_results[train][site]['accuracy'] \n",
    "               for train in domain_data.keys() if train != site]\n",
    "    comparison_data.append({\n",
    "        'Site': site,\n",
    "        'Scenario': 'Out-Domain',\n",
    "        'Accuracy': np.mean(out_accs)\n",
    "    })\n",
    "    \n",
    "    # Cross-domain\n",
    "    comparison_data.append({\n",
    "        'Site': site,\n",
    "        'Scenario': 'Cross-Domain',\n",
    "        'Accuracy': cross_domain_results[site]['accuracy']\n",
    "    })\n",
    "\n",
    "df_comp = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Visualization\n",
    "fig = px.bar(df_comp, x='Site', y='Accuracy', color='Scenario',\n",
    "             title='Performance Comparison: In-Domain vs Out-Domain vs Cross-Domain',\n",
    "             barmode='group', height=500,\n",
    "             color_discrete_map={\n",
    "                 'In-Domain': '#2E7D32',\n",
    "                 'Out-Domain': '#F57C00',\n",
    "                 'Cross-Domain': '#1976D2'\n",
    "             })\n",
    "fig.update_layout(yaxis_range=[0, 1])\n",
    "fig.show()\n",
    "\n",
    "# Print statistics\n",
    "for scenario in ['In-Domain', 'Out-Domain', 'Cross-Domain']:\n",
    "    data = df_comp[df_comp['Scenario'] == scenario]['Accuracy']\n",
    "    print(f\"\\n{scenario}:\")\n",
    "    print(f\"  Mean: {data.mean():.4f}\")\n",
    "    print(f\"  Std:  {data.std():.4f}\")\n",
    "    print(f\"  Range: [{data.min():.4f}, {data.max():.4f}]\")\n",
    "\n",
    "# Domain shift analysis\n",
    "in_mean = df_comp[df_comp['Scenario'] == 'In-Domain']['Accuracy'].mean()\n",
    "out_mean = df_comp[df_comp['Scenario'] == 'Out-Domain']['Accuracy'].mean()\n",
    "cross_mean = df_comp[df_comp['Scenario'] == 'Cross-Domain']['Accuracy'].mean()\n",
    "\n",
    "print(\"\\nüìâ Domain Shift Effects:\")\n",
    "print(f\"  In‚ÜíOut drop: {(in_mean - out_mean):.4f} ({(in_mean - out_mean)/in_mean*100:.1f}%)\")\n",
    "print(f\"  In‚ÜíCross drop: {(in_mean - cross_mean):.4f} ({(in_mean - cross_mean)/in_mean*100:.1f}%)\")\n",
    "print(f\"  Cross vs Out gain: {(cross_mean - out_mean):.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Analysis Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}