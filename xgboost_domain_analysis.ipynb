{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost In-Domain vs Out-Domain Performance Analysis\n",
    "\n",
    "This notebook analyzes XGBoost performance across different domains (sites) using the VA dataset.\n",
    "\n",
    "## Dataset Information\n",
    "- File: `adult_numeric_20250729_155457.csv`\n",
    "- Target: `va34` (34 classes)\n",
    "- Sites: AP, Bohol, Dar, Mexico, Pemba, UP\n",
    "- Features: 169 (after dropping 'cod5' and 'site')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install xgboost scikit-learn pandas numpy matplotlib seaborn plotly -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# For Google Colab: Upload the file using the file browser or mount Google Drive\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n",
    "df = pd.read_csv('processed_data/adult_numeric_20250729_155457.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names: {list(df.columns[:10])}...\")  # Show first 10 columns\n",
    "print(f\"\\nSites distribution:\")\n",
    "print(df['site'].value_counts())\n",
    "print(f\"\\nTarget classes: {df['va34'].nunique()} unique classes\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "# Drop 'cod5' column as specified\n",
    "if 'cod5' in df.columns:\n",
    "    df_clean = df.drop('cod5', axis=1)\n",
    "    print(\"Dropped 'cod5' column\")\n",
    "else:\n",
    "    df_clean = df.copy()\n",
    "    print(\"'cod5' column not found, proceeding with all columns\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df_clean.drop(['va34', 'site'], axis=1)\n",
    "y = df_clean['va34']\n",
    "sites = df_clean['site']\n",
    "\n",
    "print(f\"\\nFeatures shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nTarget value distribution (top 10):\")\n",
    "print(y.value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze domain statistics\n",
    "site_stats = df_clean.groupby('site').agg({\n",
    "    'va34': ['count', 'nunique', lambda x: x.value_counts().iloc[0]]\n",
    "}).round(2)\n",
    "site_stats.columns = ['Sample_Count', 'Unique_Classes', 'Most_Common_Class_Count']\n",
    "site_stats['Class_Coverage'] = site_stats['Unique_Classes'] / df_clean['va34'].nunique()\n",
    "\n",
    "print(\"Site Statistics:\")\n",
    "print(site_stats)\n",
    "\n",
    "# Visualize site distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Samples per site\n",
    "axes[0].bar(site_stats.index, site_stats['Sample_Count'], color='skyblue', alpha=0.7)\n",
    "axes[0].set_title('Samples per Site')\n",
    "axes[0].set_ylabel('Number of Samples')\n",
    "axes[0].set_xlabel('Site')\n",
    "for i, v in enumerate(site_stats['Sample_Count']):\n",
    "    axes[0].text(i, v + 20, str(int(v)), ha='center')\n",
    "\n",
    "# Classes per site\n",
    "axes[1].bar(site_stats.index, site_stats['Unique_Classes'], color='lightcoral', alpha=0.7)\n",
    "axes[1].set_title('Unique Classes per Site')\n",
    "axes[1].set_ylabel('Number of Unique Classes')\n",
    "axes[1].set_xlabel('Site')\n",
    "for i, v in enumerate(site_stats['Unique_Classes']):\n",
    "    axes[1].text(i, v + 0.5, str(int(v)), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Domain Splitting Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_domain_splits(df, test_size=0.2, min_samples=50, random_state=42):\n",
    "    \"\"\"Create train/test splits for each site\"\"\"\n",
    "    domain_splits = {}\n",
    "    \n",
    "    for site in df['site'].unique():\n",
    "        site_data = df[df['site'] == site]\n",
    "        X_site = site_data.drop(['va34', 'site'], axis=1)\n",
    "        y_site = site_data['va34']\n",
    "        \n",
    "        # Check if site has enough samples\n",
    "        if len(site_data) < min_samples:\n",
    "            print(f\"Warning: {site} has only {len(site_data)} samples (< {min_samples}), using all for training\")\n",
    "            domain_splits[site] = {\n",
    "                'X_train': X_site, 'X_test': X_site[:10],  # Small test set for consistency\n",
    "                'y_train': y_site, 'y_test': y_site[:10],\n",
    "                'full_X': X_site, 'full_y': y_site\n",
    "            }\n",
    "            continue\n",
    "        \n",
    "        # Check if we can stratify\n",
    "        min_class_count = y_site.value_counts().min()\n",
    "        if min_class_count >= 2:\n",
    "            # Stratified split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_site, y_site, test_size=test_size, \n",
    "                random_state=random_state, stratify=y_site\n",
    "            )\n",
    "        else:\n",
    "            # Regular split without stratification\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_site, y_site, test_size=test_size, \n",
    "                random_state=random_state\n",
    "            )\n",
    "        \n",
    "        domain_splits[site] = {\n",
    "            'X_train': X_train, 'X_test': X_test,\n",
    "            'y_train': y_train, 'y_test': y_test,\n",
    "            'full_X': X_site, 'full_y': y_site\n",
    "        }\n",
    "    \n",
    "    return domain_splits\n",
    "\n",
    "# Create domain splits\n",
    "domain_data = create_domain_splits(df_clean)\n",
    "\n",
    "print(f\"Created splits for {len(domain_data)} sites:\")\n",
    "for site, data in domain_data.items():\n",
    "    print(f\"{site:10} - Train: {len(data['X_train']):4}, Test: {len(data['X_test']):4}, Total: {len(data['full_X']):4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_model(X_train, y_train, params=None, verbose=False):\n",
    "    \"\"\"Train XGBoost model with regularization for better generalization\"\"\"\n",
    "    \n",
    "    # Encode labels to ensure they are 0-indexed\n",
    "    le = LabelEncoder()\n",
    "    y_train_encoded = le.fit_transform(y_train)\n",
    "    \n",
    "    if params is None:\n",
    "        params = {\n",
    "            'objective': 'multi:softprob',\n",
    "            'num_class': len(np.unique(y_train_encoded)),\n",
    "            'max_depth': 4,  # Shallow trees for better generalization\n",
    "            'learning_rate': 0.1,\n",
    "            'n_estimators': 100,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'reg_alpha': 0.1,  # L1 regularization\n",
    "            'reg_lambda': 1.0,  # L2 regularization\n",
    "            'random_state': 42,\n",
    "            'verbosity': 0\n",
    "        }\n",
    "    \n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train_encoded, verbose=verbose)\n",
    "    \n",
    "    return model, le\n",
    "\n",
    "def evaluate_model(model, label_encoder, X_test, y_test):\n",
    "    \"\"\"Evaluate model and return multiple metrics\"\"\"\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "    balanced_acc = balanced_accuracy_score(y_test_encoded, y_pred)\n",
    "    f1_macro = f1_score(y_test_encoded, y_pred, average='macro', zero_division=0)\n",
    "    f1_weighted = f1_score(y_test_encoded, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'predictions': y_pred,\n",
    "        'true_labels': y_test_encoded\n",
    "    }\n",
    "\n",
    "print(\"Model training functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. In-Domain Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_in_domain_performance(domain_data):\n",
    "    \"\"\"Evaluate in-domain performance for each site\"\"\"\n",
    "    in_domain_results = {}\n",
    "    \n",
    "    print(\"Training in-domain models...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for site, data in domain_data.items():\n",
    "        print(f\"\\nProcessing {site}...\")\n",
    "        \n",
    "        # Train model on site's training data\n",
    "        model, le = train_xgboost_model(data['X_train'], data['y_train'])\n",
    "        \n",
    "        # Test on same site's test data\n",
    "        results = evaluate_model(model, le, data['X_test'], data['y_test'])\n",
    "        \n",
    "        in_domain_results[site] = {\n",
    "            'model': model,\n",
    "            'label_encoder': le,\n",
    "            'accuracy': results['accuracy'],\n",
    "            'balanced_accuracy': results['balanced_accuracy'],\n",
    "            'f1_macro': results['f1_macro'],\n",
    "            'f1_weighted': results['f1_weighted'],\n",
    "            'predictions': results['predictions'],\n",
    "            'true_labels': results['true_labels']\n",
    "        }\n",
    "        \n",
    "        print(f\"  Accuracy: {results['accuracy']:.4f}\")\n",
    "        print(f\"  Balanced Accuracy: {results['balanced_accuracy']:.4f}\")\n",
    "        print(f\"  F1 Macro: {results['f1_macro']:.4f}\")\n",
    "    \n",
    "    return in_domain_results\n",
    "\n",
    "# Evaluate in-domain performance\n",
    "in_domain_results = evaluate_in_domain_performance(domain_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize in-domain performance\n",
    "sites = list(in_domain_results.keys())\n",
    "metrics = ['accuracy', 'balanced_accuracy', 'f1_macro', 'f1_weighted']\n",
    "\n",
    "# Create comparison dataframe\n",
    "in_domain_df = pd.DataFrame({\n",
    "    site: {\n",
    "        metric: in_domain_results[site][metric] \n",
    "        for metric in metrics\n",
    "    }\n",
    "    for site in sites\n",
    "}).T\n",
    "\n",
    "# Plot in-domain performance\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "in_domain_df.plot(kind='bar', ax=ax, width=0.8)\n",
    "ax.set_title('In-Domain Performance by Site', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xlabel('Site')\n",
    "ax.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nIn-Domain Performance Summary:\")\n",
    "print(in_domain_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Out-Domain Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_out_domain_performance(domain_data):\n",
    "    \"\"\"Evaluate out-domain performance (train on one site, test on others)\"\"\"\n",
    "    out_domain_results = {}\n",
    "    \n",
    "    sites = list(domain_data.keys())\n",
    "    \n",
    "    print(\"Evaluating out-domain performance...\")\n",
    "    print(\"(Training on each site and testing on all others)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for train_site in sites:\n",
    "        out_domain_results[train_site] = {}\n",
    "        train_data = domain_data[train_site]\n",
    "        \n",
    "        # Train model on one site using full data\n",
    "        print(f\"\\nTraining on {train_site}...\")\n",
    "        model, le = train_xgboost_model(train_data['full_X'], train_data['full_y'])\n",
    "        \n",
    "        for test_site in sites:\n",
    "            if train_site == test_site:\n",
    "                continue  # Skip same-domain evaluation\n",
    "            \n",
    "            test_data = domain_data[test_site]\n",
    "            \n",
    "            # Handle unseen labels in test set\n",
    "            try:\n",
    "                results = evaluate_model(model, le, test_data['full_X'], test_data['full_y'])\n",
    "                accuracy = results['accuracy']\n",
    "            except ValueError as e:\n",
    "                # If test set has unseen labels, set accuracy to 0\n",
    "                print(f\"  Warning: {train_site} -> {test_site}: Unseen labels in test set\")\n",
    "                accuracy = 0.0\n",
    "                results = {'accuracy': 0.0, 'balanced_accuracy': 0.0, \n",
    "                          'f1_macro': 0.0, 'f1_weighted': 0.0}\n",
    "            \n",
    "            out_domain_results[train_site][test_site] = results\n",
    "            print(f\"  {train_site} -> {test_site}: {accuracy:.4f}\")\n",
    "    \n",
    "    return out_domain_results\n",
    "\n",
    "# Evaluate out-domain performance\n",
    "out_domain_results = evaluate_out_domain_performance(domain_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create out-domain performance matrix\n",
    "sites = list(domain_data.keys())\n",
    "out_domain_matrix = np.zeros((len(sites), len(sites)))\n",
    "\n",
    "for i, train_site in enumerate(sites):\n",
    "    for j, test_site in enumerate(sites):\n",
    "        if train_site == test_site:\n",
    "            # Use in-domain accuracy for diagonal\n",
    "            out_domain_matrix[i, j] = in_domain_results[train_site]['accuracy']\n",
    "        else:\n",
    "            out_domain_matrix[i, j] = out_domain_results[train_site][test_site]['accuracy']\n",
    "\n",
    "# Visualize out-domain performance matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(out_domain_matrix, annot=True, fmt='.3f', \n",
    "            xticklabels=sites, yticklabels=sites, \n",
    "            cmap='RdYlGn', vmin=0, vmax=1, cbar_kws={'label': 'Accuracy'})\n",
    "plt.title('Domain Transfer Performance Matrix\\n(Train Site → Test Site)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Train Site')\n",
    "plt.xlabel('Test Site')\n",
    "\n",
    "# Add diagonal line to highlight in-domain performance\n",
    "for i in range(len(sites)):\n",
    "    plt.gca().add_patch(plt.Rectangle((i, i), 1, 1, fill=False, edgecolor='blue', lw=2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: Blue boxes indicate in-domain performance (diagonal)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross-Domain Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cross_domain_performance(domain_data):\n",
    "    \"\"\"Train on multiple sites, test on held-out site\"\"\"\n",
    "    cross_domain_results = {}\n",
    "    sites = list(domain_data.keys())\n",
    "    \n",
    "    print(\"Evaluating cross-domain performance...\")\n",
    "    print(\"(Training on all sites except one, testing on held-out site)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for held_out_site in sites:\n",
    "        # Combine data from all other sites for training\n",
    "        train_sites = [s for s in sites if s != held_out_site]\n",
    "        \n",
    "        X_train_combined = []\n",
    "        y_train_combined = []\n",
    "        \n",
    "        for site in train_sites:\n",
    "            X_train_combined.append(domain_data[site]['full_X'])\n",
    "            y_train_combined.append(domain_data[site]['full_y'])\n",
    "        \n",
    "        X_train = pd.concat(X_train_combined, axis=0)\n",
    "        y_train = pd.concat(y_train_combined, axis=0)\n",
    "        \n",
    "        print(f\"\\nTraining on {', '.join(train_sites)}\")\n",
    "        print(f\"  Combined training size: {len(X_train)} samples\")\n",
    "        \n",
    "        # Train model on combined data\n",
    "        model, le = train_xgboost_model(X_train, y_train)\n",
    "        \n",
    "        # Test on held-out site\n",
    "        test_data = domain_data[held_out_site]\n",
    "        \n",
    "        try:\n",
    "            results = evaluate_model(model, le, test_data['full_X'], test_data['full_y'])\n",
    "        except ValueError as e:\n",
    "            print(f\"  Warning: Unseen labels in {held_out_site}\")\n",
    "            results = {'accuracy': 0.0, 'balanced_accuracy': 0.0, \n",
    "                      'f1_macro': 0.0, 'f1_weighted': 0.0}\n",
    "        \n",
    "        cross_domain_results[held_out_site] = {\n",
    "            'model': model,\n",
    "            'label_encoder': le,\n",
    "            'accuracy': results['accuracy'],\n",
    "            'balanced_accuracy': results['balanced_accuracy'],\n",
    "            'f1_macro': results['f1_macro'],\n",
    "            'f1_weighted': results['f1_weighted'],\n",
    "            'train_sites': train_sites\n",
    "        }\n",
    "        \n",
    "        print(f\"  Testing on {held_out_site}: Accuracy = {results['accuracy']:.4f}\")\n",
    "    \n",
    "    return cross_domain_results\n",
    "\n",
    "# Evaluate cross-domain performance\n",
    "cross_domain_results = evaluate_cross_domain_performance(domain_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comprehensive Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive performance comparison\n",
    "sites = list(domain_data.keys())\n",
    "\n",
    "# Collect all performance metrics\n",
    "performance_data = []\n",
    "\n",
    "# In-domain performance\n",
    "for site in sites:\n",
    "    performance_data.append({\n",
    "        'Site': site,\n",
    "        'Scenario': 'In-Domain',\n",
    "        'Accuracy': in_domain_results[site]['accuracy'],\n",
    "        'Balanced_Accuracy': in_domain_results[site]['balanced_accuracy'],\n",
    "        'F1_Macro': in_domain_results[site]['f1_macro']\n",
    "    })\n",
    "\n",
    "# Out-domain performance (average across all target sites)\n",
    "for train_site in sites:\n",
    "    out_accuracies = [out_domain_results[train_site][test_site]['accuracy'] \n",
    "                     for test_site in sites if test_site != train_site]\n",
    "    out_balanced = [out_domain_results[train_site][test_site]['balanced_accuracy'] \n",
    "                   for test_site in sites if test_site != train_site]\n",
    "    out_f1 = [out_domain_results[train_site][test_site]['f1_macro'] \n",
    "             for test_site in sites if test_site != train_site]\n",
    "    \n",
    "    performance_data.append({\n",
    "        'Site': train_site,\n",
    "        'Scenario': 'Out-Domain (Avg)',\n",
    "        'Accuracy': np.mean(out_accuracies),\n",
    "        'Balanced_Accuracy': np.mean(out_balanced),\n",
    "        'F1_Macro': np.mean(out_f1)\n",
    "    })\n",
    "\n",
    "# Cross-domain performance\n",
    "for site in sites:\n",
    "    performance_data.append({\n",
    "        'Site': site,\n",
    "        'Scenario': 'Cross-Domain',\n",
    "        'Accuracy': cross_domain_results[site]['accuracy'],\n",
    "        'Balanced_Accuracy': cross_domain_results[site]['balanced_accuracy'],\n",
    "        'F1_Macro': cross_domain_results[site]['f1_macro']\n",
    "    })\n",
    "\n",
    "df_performance = pd.DataFrame(performance_data)\n",
    "\n",
    "# Create interactive comparison plot using plotly\n",
    "fig = px.bar(df_performance, x='Site', y='Accuracy', color='Scenario',\n",
    "             title='Performance Comparison: In-Domain vs Out-Domain vs Cross-Domain',\n",
    "             barmode='group', height=500,\n",
    "             color_discrete_map={\n",
    "                 'In-Domain': '#2E7D32',\n",
    "                 'Out-Domain (Avg)': '#F57C00',\n",
    "                 'Cross-Domain': '#1976D2'\n",
    "             })\n",
    "fig.update_layout(xaxis_tickangle=-45)\n",
    "fig.update_yaxis(range=[0, 1])\n",
    "fig.show()\n",
    "\n",
    "# Print performance summary table\n",
    "print(\"\\nPerformance Summary (Accuracy):\")\n",
    "performance_pivot = df_performance.pivot(index='Site', columns='Scenario', values='Accuracy')\n",
    "print(performance_pivot.round(4))\n",
    "\n",
    "# Calculate average performance across all sites\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Average Performance Across All Sites:\")\n",
    "print(\"=\"*60)\n",
    "avg_performance = df_performance.groupby('Scenario')[['Accuracy', 'Balanced_Accuracy', 'F1_Macro']].mean()\n",
    "print(avg_performance.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Domain Shift Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze domain shift effects\n",
    "sites = list(domain_data.keys())\n",
    "\n",
    "# Calculate performance drops\n",
    "performance_drops = {}\n",
    "for site in sites:\n",
    "    in_domain_acc = in_domain_results[site]['accuracy']\n",
    "    cross_domain_acc = cross_domain_results[site]['accuracy']\n",
    "    \n",
    "    # Calculate average out-domain accuracy when this site is the test site\n",
    "    out_domain_accs = [out_domain_results[train_site][site]['accuracy'] \n",
    "                      for train_site in sites if train_site != site]\n",
    "    avg_out_domain_acc = np.mean(out_domain_accs) if out_domain_accs else 0\n",
    "    \n",
    "    performance_drops[site] = {\n",
    "        'In-Domain': in_domain_acc,\n",
    "        'Cross-Domain': cross_domain_acc,\n",
    "        'Out-Domain (Avg)': avg_out_domain_acc,\n",
    "        'In→Cross Drop': in_domain_acc - cross_domain_acc,\n",
    "        'In→Out Drop': in_domain_acc - avg_out_domain_acc,\n",
    "        'Relative Drop (%)': ((in_domain_acc - cross_domain_acc) / in_domain_acc * 100) if in_domain_acc > 0 else 0\n",
    "    }\n",
    "\n",
    "drops_df = pd.DataFrame(performance_drops).T\n",
    "\n",
    "print(\"Domain Shift Analysis:\")\n",
    "print(\"=\"*80)\n",
    "print(drops_df.round(4))\n",
    "\n",
    "# Visualize performance drops\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Performance drops\n",
    "x = np.arange(len(sites))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, drops_df['In→Cross Drop'], width, \n",
    "           label='In-Domain → Cross-Domain Drop', color='#FF6B6B', alpha=0.7)\n",
    "axes[0].bar(x + width/2, drops_df['In→Out Drop'], width, \n",
    "           label='In-Domain → Out-Domain Drop', color='#4ECDC4', alpha=0.7)\n",
    "axes[0].set_xlabel('Site')\n",
    "axes[0].set_ylabel('Accuracy Drop')\n",
    "axes[0].set_title('Performance Degradation due to Domain Shift', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(sites)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Plot 2: Absolute performance comparison\n",
    "axes[1].plot(sites, drops_df['In-Domain'], 'o-', label='In-Domain', \n",
    "            linewidth=2, markersize=8, color='#2E7D32')\n",
    "axes[1].plot(sites, drops_df['Cross-Domain'], 's-', label='Cross-Domain', \n",
    "            linewidth=2, markersize=8, color='#1976D2')\n",
    "axes[1].plot(sites, drops_df['Out-Domain (Avg)'], '^-', label='Out-Domain (Avg)', \n",
    "            linewidth=2, markersize=8, color='#F57C00')\n",
    "axes[1].set_xlabel('Site')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Absolute Performance Across Different Scenarios', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Summary Statistics:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Average In-Domain Accuracy: {drops_df['In-Domain'].mean():.4f} ± {drops_df['In-Domain'].std():.4f}\")\n",
    "print(f\"Average Cross-Domain Accuracy: {drops_df['Cross-Domain'].mean():.4f} ± {drops_df['Cross-Domain'].std():.4f}\")\n",
    "print(f\"Average Out-Domain Accuracy: {drops_df['Out-Domain (Avg)'].mean():.4f} ± {drops_df['Out-Domain (Avg)'].std():.4f}\")\n",
    "print(f\"\\nAverage Performance Drop:\")\n",
    "print(f\"  In→Cross: {drops_df['In→Cross Drop'].mean():.4f} ({drops_df['Relative Drop (%)'].mean():.1f}%)\")\n",
    "print(f\"  In→Out: {drops_df['In→Out Drop'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance across different models\n",
    "feature_names = list(domain_data[list(domain_data.keys())[0]]['full_X'].columns)\n",
    "\n",
    "# Collect feature importance from in-domain and cross-domain models\n",
    "importance_data = {}\n",
    "\n",
    "# In-domain models\n",
    "for site in domain_data.keys():\n",
    "    model = in_domain_results[site]['model']\n",
    "    importance_data[f'{site}_in'] = model.feature_importances_\n",
    "\n",
    "# Cross-domain models\n",
    "for site in cross_domain_results.keys():\n",
    "    model = cross_domain_results[site]['model']\n",
    "    importance_data[f'{site}_cross'] = model.feature_importances_\n",
    "\n",
    "# Create importance DataFrame\n",
    "importance_df = pd.DataFrame(importance_data, index=feature_names)\n",
    "\n",
    "# Calculate average importance\n",
    "avg_importance_in = importance_df[[col for col in importance_df.columns if '_in' in col]].mean(axis=1)\n",
    "avg_importance_cross = importance_df[[col for col in importance_df.columns if '_cross' in col]].mean(axis=1)\n",
    "overall_avg = importance_df.mean(axis=1)\n",
    "\n",
    "# Get top features\n",
    "top_n = 20\n",
    "top_features = overall_avg.nlargest(top_n)\n",
    "\n",
    "# Plot feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Top features overall\n",
    "axes[0].barh(range(len(top_features)), top_features.values, color='steelblue', alpha=0.7)\n",
    "axes[0].set_yticks(range(len(top_features)))\n",
    "axes[0].set_yticklabels(top_features.index)\n",
    "axes[0].set_xlabel('Average Feature Importance')\n",
    "axes[0].set_title(f'Top {top_n} Most Important Features (Overall)', fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Compare in-domain vs cross-domain importance for top features\n",
    "top_feature_names = top_features.index\n",
    "in_domain_imp = avg_importance_in[top_feature_names].values\n",
    "cross_domain_imp = avg_importance_cross[top_feature_names].values\n",
    "\n",
    "x_pos = np.arange(len(top_feature_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[1].barh(x_pos - width/2, in_domain_imp, width, label='In-Domain', color='#2E7D32', alpha=0.7)\n",
    "axes[1].barh(x_pos + width/2, cross_domain_imp, width, label='Cross-Domain', color='#1976D2', alpha=0.7)\n",
    "axes[1].set_yticks(x_pos)\n",
    "axes[1].set_yticklabels(top_feature_names)\n",
    "axes[1].set_xlabel('Average Feature Importance')\n",
    "axes[1].set_title('Feature Importance: In-Domain vs Cross-Domain', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate feature stability (variance across models)\n",
    "feature_stability = importance_df.std(axis=1).sort_values()\n",
    "print(f\"\\nMost stable features (lowest variance across models):\")\n",
    "print(feature_stability.head(10).round(6))\n",
    "\n",
    "print(f\"\\nMost variable features (highest variance across models):\")\n",
    "print(feature_stability.tail(10).round(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Report and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive final report\n",
    "print(\"=\"*80)\n",
    "print(\" \"*20 + \"XGBOOST DOMAIN ANALYSIS FINAL REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n📊 DATASET OVERVIEW:\")\n",
    "print(f\"  • Total samples: {len(df_clean):,}\")\n",
    "print(f\"  • Number of features: {len(X.columns)}\")\n",
    "print(f\"  • Number of classes: {y.nunique()}\")\n",
    "print(f\"  • Number of sites: {len(sites)}\")\n",
    "\n",
    "print(f\"\\n🌍 SITE DISTRIBUTION:\")\n",
    "for site in sites:\n",
    "    count = len(domain_data[site]['full_X'])\n",
    "    percentage = count/len(df_clean)*100\n",
    "    print(f\"  • {site:10}: {count:,} samples ({percentage:.1f}%)\")\n",
    "\n",
    "# Calculate overall statistics\n",
    "in_domain_scores = [in_domain_results[site]['accuracy'] for site in sites]\n",
    "cross_domain_scores = [cross_domain_results[site]['accuracy'] for site in sites]\n",
    "out_domain_all = []\n",
    "for train_site in sites:\n",
    "    for test_site in sites:\n",
    "        if train_site != test_site:\n",
    "            out_domain_all.append(out_domain_results[train_site][test_site]['accuracy'])\n",
    "\n",
    "print(f\"\\n📈 PERFORMANCE SUMMARY:\")\n",
    "print(f\"  • Average In-Domain Accuracy:    {np.mean(in_domain_scores):.4f} ± {np.std(in_domain_scores):.4f}\")\n",
    "print(f\"  • Average Cross-Domain Accuracy: {np.mean(cross_domain_scores):.4f} ± {np.std(cross_domain_scores):.4f}\")\n",
    "print(f\"  • Average Out-Domain Accuracy:   {np.mean(out_domain_all):.4f} ± {np.std(out_domain_all):.4f}\")\n",
    "\n",
    "print(f\"\\n📉 PERFORMANCE DEGRADATION:\")\n",
    "in_cross_drop = np.mean(in_domain_scores) - np.mean(cross_domain_scores)\n",
    "in_out_drop = np.mean(in_domain_scores) - np.mean(out_domain_all)\n",
    "print(f\"  • In-Domain → Cross-Domain: -{in_cross_drop:.4f} ({in_cross_drop/np.mean(in_domain_scores)*100:.1f}% relative drop)\")\n",
    "print(f\"  • In-Domain → Out-Domain:   -{in_out_drop:.4f} ({in_out_drop/np.mean(in_domain_scores)*100:.1f}% relative drop)\")\n",
    "\n",
    "print(f\"\\n🏆 BEST & WORST PERFORMERS:\")\n",
    "best_in_domain = max(sites, key=lambda x: in_domain_results[x]['accuracy'])\n",
    "worst_in_domain = min(sites, key=lambda x: in_domain_results[x]['accuracy'])\n",
    "best_cross_domain = max(sites, key=lambda x: cross_domain_results[x]['accuracy'])\n",
    "worst_cross_domain = min(sites, key=lambda x: cross_domain_results[x]['accuracy'])\n",
    "\n",
    "print(f\"  In-Domain:\")\n",
    "print(f\"    • Best:  {best_in_domain} ({in_domain_results[best_in_domain]['accuracy']:.4f})\")\n",
    "print(f\"    • Worst: {worst_in_domain} ({in_domain_results[worst_in_domain]['accuracy']:.4f})\")\n",
    "print(f\"  Cross-Domain:\")\n",
    "print(f\"    • Best:  {best_cross_domain} ({cross_domain_results[best_cross_domain]['accuracy']:.4f})\")\n",
    "print(f\"    • Worst: {worst_cross_domain} ({cross_domain_results[worst_cross_domain]['accuracy']:.4f})\")\n",
    "\n",
    "# Find best and worst domain pairs\n",
    "best_pair = None\n",
    "best_pair_score = 0\n",
    "worst_pair = None\n",
    "worst_pair_score = 1\n",
    "\n",
    "for train_site in sites:\n",
    "    for test_site in sites:\n",
    "        if train_site != test_site:\n",
    "            score = out_domain_results[train_site][test_site]['accuracy']\n",
    "            if score > best_pair_score:\n",
    "                best_pair_score = score\n",
    "                best_pair = (train_site, test_site)\n",
    "            if score < worst_pair_score:\n",
    "                worst_pair_score = score\n",
    "                worst_pair = (train_site, test_site)\n",
    "\n",
    "print(f\"\\n🔄 DOMAIN TRANSFER PAIRS:\")\n",
    "print(f\"  • Best transfer:  {best_pair[0]} → {best_pair[1]} ({best_pair_score:.4f})\")\n",
    "print(f\"  • Worst transfer: {worst_pair[0]} → {worst_pair[1]} ({worst_pair_score:.4f})\")\n",
    "\n",
    "print(f\"\\n🔑 TOP 5 MOST IMPORTANT FEATURES:\")\n",
    "for i, (feature, importance) in enumerate(top_features.head().items(), 1):\n",
    "    print(f\"  {i}. {feature}: {importance:.4f}\")\n",
    "\n",
    "print(f\"\\n💡 KEY INSIGHTS:\")\n",
    "print(f\"  1. Cross-domain training (multiple sites) improves generalization by {(np.mean(cross_domain_scores) - np.mean(out_domain_all)):.4f}\")\n",
    "print(f\"  2. Domain shift causes an average {in_cross_drop/np.mean(in_domain_scores)*100:.1f}% performance drop\")\n",
    "print(f\"  3. Site '{best_cross_domain}' shows best robustness to domain shift\")\n",
    "print(f\"  4. Site '{worst_cross_domain}' is most affected by domain shift\")\n",
    "\n",
    "if best_pair_score > 0.5:\n",
    "    print(f\"  5. Sites '{best_pair[0]}' and '{best_pair[1]}' have good domain similarity\")\n",
    "else:\n",
    "    print(f\"  5. All domain pairs show significant distribution shift\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Analysis Complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to files\n",
    "save_results = input(\"Do you want to save the results? (y/n): \").lower() == 'y'\n",
    "\n",
    "if save_results:\n",
    "    # Create results directory\n",
    "    import os\n",
    "    os.makedirs('xgboost_domain_results', exist_ok=True)\n",
    "    \n",
    "    # Save performance summary\n",
    "    df_performance.to_csv('xgboost_domain_results/performance_summary.csv', index=False)\n",
    "    print(\"✓ Saved performance_summary.csv\")\n",
    "    \n",
    "    # Save domain shift analysis\n",
    "    drops_df.to_csv('xgboost_domain_results/domain_shift_analysis.csv')\n",
    "    print(\"✓ Saved domain_shift_analysis.csv\")\n",
    "    \n",
    "    # Save feature importance\n",
    "    importance_df.to_csv('xgboost_domain_results/feature_importance.csv')\n",
    "    print(\"✓ Saved feature_importance.csv\")\n",
    "    \n",
    "    # Save out-domain matrix\n",
    "    pd.DataFrame(out_domain_matrix, index=sites, columns=sites).to_csv(\n",
    "        'xgboost_domain_results/out_domain_matrix.csv'\n",
    "    )\n",
    "    print(\"✓ Saved out_domain_matrix.csv\")\n",
    "    \n",
    "    print(\"\\nAll results saved to 'xgboost_domain_results/' directory\")\n",
    "else:\n",
    "    print(\"Results not saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}